{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A quick start for aTEAM/optim\n",
    "More test example can be found in aTEAM/test/optim*.py. \n",
    "\"\"\"\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from aTEAM.optim import NumpyFunctionInterface,ParamGroupsManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 1\n",
    "Let us start with an example.\n",
    "At first, we define a PyTorch tensor function \"powell_bs\"  \n",
    "\"\"\"\n",
    "def powell_bs(x):\n",
    "    return (1e4*x[0]*x[1]-1)**2+((-x[0]).exp()+(-x[1]).exp()-1.0001)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "And then define the variable \"nfix\" to be optimized:\n",
    "    min_{nfix} powell_bs(nfix)\n",
    "\"\"\"\n",
    "nfix = torch.tensor([0,1], dtype=torch.float64, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a interface \"forward\" for NumpyFunctionInterface is needed\n",
    "\"\"\"\n",
    "def forward():\n",
    "    return powell_bs(nfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At last, construct your NumpyFunctionInterface of the PyTorch tensor function\n",
    "\"\"\"\n",
    "listofparameters = [nfix,]\n",
    "nfi = NumpyFunctionInterface(listofparameters,forward=forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now it's ready to use interfaces given by \"nfi\": \"nfi.flat_param,nfi.f,nfi.fprime\". \n",
    "What these interfaces do is something like\n",
    "```\n",
    "class NumpyFunctionInterface:\n",
    "    @property\n",
    "    def params(self):\n",
    "    # notice that nfi = NumpyFunctionInterface(listofparameters,forward)\n",
    "        for p in listofparameters:\n",
    "            yield p\n",
    "    @property\n",
    "    def flat_param(self):\n",
    "        views = []\n",
    "        for p in self.params:\n",
    "            views.append(p.view(-1))\n",
    "        return torch.cat(views,0).numpy()\n",
    "    @property.setter\n",
    "    def flat_param(self,x): # x is a numpy array\n",
    "        for p in self.params: \n",
    "            p[:] = x[pidx_start:pidx_end] \n",
    "            # For simplicity here we do not show details of \n",
    "            # type conversion and subscript matching between p and x.\n",
    "    def f(self,x): \n",
    "        self.flat_param = x\n",
    "        return forward()\n",
    "    def fprime(self,x):\n",
    "        loss = self.f(x)\n",
    "        loss.backward() # Here we utilize autograd feature of PyTorch\n",
    "        grad = np.zeros(x.size)\n",
    "        for p in self.params:\n",
    "            grad[pidx_start:pidx_end] = p.grad\n",
    "        return grad\n",
    "```\n",
    "Try these commands:\n",
    "    x = np.random.randn(nfi.numel())\n",
    "    assert(np.equal(nfi.f(x),powell_bs(x)))\n",
    "    x[0] = 1\n",
    "    nfi.flat_param = x \n",
    "    # nfi.flat_param[0] = 1 is not permitted since property is not a ndarray\n",
    "    assert(np.equal(nfi.f(nfi.flat_param),powell_bs(x)))\n",
    "These interfaces enable us to use lbfgs,slsqp from scipy.optimize.\n",
    "\"\"\"\n",
    "from scipy.optimize.lbfgsb import fmin_l_bfgs_b as lbfgsb\n",
    "from scipy.optimize.slsqp import fmin_slsqp as slsqp\n",
    "x0 = array([0,1])\n",
    "print(\" ***************** powell_bs ***************** \")\n",
    "x,f,d = lbfgsb(nfi.f,x0,nfi.fprime,m=100,factr=1,pgtol=1e-14,iprint=10)\n",
    "out,fx,its,imode,smode = slsqp(nfi.f,x0,fprime=nfi.fprime,\n",
    "        acc=1e-16,iter=15000,iprint=1,full_output=True)\n",
    "print('\\noptimial solution\\n',out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Further more, if we want to impose constraint \"nfix[0] = 1e-5\" to the problem, \n",
    "we can define the projection function of \"nfix\" and its gradient: \"x_proj\",\"grad_proj\", \n",
    "and then add these hooks by call \"nfi.set_options\".\n",
    "\"nfi.f\" and \"nfi.fprime\" comes to \n",
    "```\n",
    "class NumpyFunctionInterface:\n",
    "    def _all_x_proj(self):\n",
    "        ...\n",
    "    def _all_grad_proj(self):\n",
    "        ...\n",
    "    @property\n",
    "    def flat_param(self):\n",
    "        self._all_x_proj()\n",
    "        ...\n",
    "    @property.setter\n",
    "    def flat_param(self,x):\n",
    "        ...\n",
    "        self._all_x_proj()\n",
    "    def fprime(self,x):\n",
    "        ...\n",
    "        self._all_grad_proj()\n",
    "        ...\n",
    "        return grad\n",
    "```\n",
    "\"\"\"\n",
    "def x_proj(params):\n",
    "    params[0].data[0] = 1e-5\n",
    "def grad_proj(params):\n",
    "    params[0].grad.data[0] = 0\n",
    "## one can also simply set since nfix is globally accessible\n",
    "# def x_proj(*args,**kw):\n",
    "#     nfix.data[0] = 1e-5\n",
    "# def grad_proj(*args,**kw):\n",
    "#     nfix.grad.data[0] = 0\n",
    "# nfi.set_oprions(0,x_proj=x_proj,grad_proj=grad_proj)\n",
    "paramidx = 0\n",
    "nfi.set_options(paramidx,x_proj=x_proj,grad_proj=grad_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can solve this constraint optimization problem in a unconstraint manner\n",
    "\"\"\"\n",
    "print(\"***************** constraint powell_bs ***************** \")\n",
    "x,f,d = lbfgsb(nfi.f,x0,nfi.fprime,m=100,factr=1,pgtol=1e-14,iprint=10)\n",
    "out,fx,its,imode,smode = slsqp(nfi.f,x0,fprime=nfi.fprime,\n",
    "        acc=1e-16,iter=15000,iprint=1,full_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The original output ('x' or 'out') of the optimizer may not satisfy the constraint.\n",
    "Recall that the nfi.flat_param will automatically do the projection in reader and setter,\n",
    "```\n",
    "class NumpyFunctionInterface:\n",
    "    @property\n",
    "    def flat_param(self):\n",
    "        self._all_x_proj()\n",
    "        views = []\n",
    "        for p in self.params:\n",
    "            views.append(p.view(-1))\n",
    "        return torch.cat(views,0).numpy()\n",
    "    @property.setter\n",
    "    def flat_param(self,x): # x is a numpy array\n",
    "        for p in self.params: \n",
    "            p[:] = x[pidx_start:pidx_end] \n",
    "        self._all_x_proj()\n",
    "```\n",
    "so we can obtain a constraint gauranteed solution by \n",
    "    out = nfi.flat_param\n",
    "\"\"\"\n",
    "out = nfi.flat_param\n",
    "print('optimial solution',out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 2\n",
    "To further understand \"NumpyFunctionInterface\", let us extend \"powell_bs\" \n",
    "to a PyTorch custom module \n",
    "(see https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html?highlight=custom)\n",
    "At first, define a pytorch module \"penalty=Penalty(100,1e-5)\"\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class Penalty(nn.Module):\n",
    "    def __init__(self,n,alpha=1e-5):\n",
    "        super(Penalty,self).__init__()\n",
    "        m = n//2\n",
    "        x1 = torch.arange(1,m+1).to(torch.float64)\n",
    "        x2 = torch.arange(m+1,n+1).to(torch.float64)\n",
    "        self.x1 = nn.Parameter(x1)\n",
    "        self.x2 = nn.Parameter(x2)\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "    def forward(self):\n",
    "        x = torch.cat([self.x1.cpu(),self.x2.cpu()],0)\n",
    "        return self.alpha*((x-1)**2).sum()+((x**2).sum()-0.25)**2\n",
    "penalty = Penalty(5,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Consider a constraint optimization problem\n",
    "    min_{penalty.x1,penalty.x2, s.t. penalty.x2[0]=1} penalty.forward()\n",
    "Then, construct \"NumpyFunctionInterface\" for this problem (each of the following way is OK)\n",
    "    # method 0 # penalty.x2 is globally accessible\n",
    "    def x_proj(*args,**kw):\n",
    "        penalty.x2.data[0] = 1e-5\n",
    "    def grad_proj(*args,**kw):\n",
    "        penalty.x2.grad.data[0] = 0\n",
    "    nfi = NumpyFunctionInterface(penalty.parameters(),forward=penalty.forward,\n",
    "        x_proj=x_proj,grad_proj=grad_proj)\n",
    "    # method 1\n",
    "    def x_proj(params_of_param_group):\n",
    "        params_of_param_group[0].data[0] = 1e-5\n",
    "    def grad_proj(params_of_param_group):\n",
    "        params_of_param_group[0].grad.data[0] = 0\n",
    "    nfi = NumpyFunctionInterface([\n",
    "        dict(params=[penalty.x1,]),\n",
    "        dict(params=[penalty.x2,],x_proj=x_proj,grad_proj=grad_proj)\n",
    "        ], penalty.forward)\n",
    "    # method 2\n",
    "    def x_proj(params_of_param_group):\n",
    "        params_of_param_group[1].data[0] = 1e-5\n",
    "    def grad_proj(params_of_param_group):\n",
    "        params_of_param_group[1].grad.data[0] = 0\n",
    "    nfi = NumpyFunctionInterface([\n",
    "        dict(params=[penalty.x1,penalty.x2],x_proj=x_proj,grad_proj=grad_proj),\n",
    "        ], penalty.forward)\n",
    "    # method 3\n",
    "    def x_proj(params_of_param_group):\n",
    "        params_of_param_group[1].data[0] = 1e-5\n",
    "    def grad_proj(params_of_param_group):\n",
    "        params_of_param_group[1].grad.data[0] = 0\n",
    "    nfi = NumpyFunctionInterface([penalty.x1,penalty.x2], penalty.forward)\n",
    "    nfi.set_options(0, x_proj=x_proj, grad_proj=grad_proj)\n",
    "In \"NumpyFunctionInterface\", parameters are devided into different parameter groups,\n",
    "any parameter groups is a dict of \n",
    "\"\"\"\n",
    "def x_proj(*args,**kw):\n",
    "    penalty.x2.data[0] = 1e-5\n",
    "def grad_proj(*args,**kw):\n",
    "    penalty.x2.grad.data[0] = 0\n",
    "nfi = NumpyFunctionInterface(penalty.parameters(),forward=penalty.forward,\n",
    "    x_proj=x_proj,grad_proj=grad_proj)\n",
    "# x0 = torch.cat([penalty.x1.cpu(),penalty.x2.cpu()],0).data.clone().numpy()\n",
    "x0 = np.random.randn(nfi.numel())\n",
    "print(\"***************** penalty *****************\")\n",
    "x,f,d = lbfgsb(nfi.f,x0,nfi.fprime,m=100,factr=1,pgtol=1e-14,iprint=10)\n",
    "out,fx,its,imode,smode = slsqp(nfi.f,x0,fprime=nfi.fprime,acc=1e-16,iter=15000,iprint=1,full_output=True)\n",
    "# the following two assignments will inforce 'out' to satisfy the constraint\n",
    "nfi.flat_param = out \n",
    "out = nfi.flat_param \n",
    "print('\\noptimial solution',out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
